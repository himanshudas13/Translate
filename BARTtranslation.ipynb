{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1_JTN7lMp1wbIx5j-M_G1GYePk2rmB286",
      "authorship_tag": "ABX9TyNFf9UmRqH21GfUX2E+x6KH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/himanshudas13/Translate/blob/main/BARTtranslation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MBartTokenizer\n",
        "from transformers import MBartForConditionalGeneration, MBartTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n"
      ],
      "metadata": {
        "id": "t18hchbGdjbr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3c71OE4YoiK",
        "outputId": "3e9b2f63-2dca-4b02-c009-a3eb9917f14b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source (Odia): ‡¨§‡≠Å‡¨Æ‡≠á ‡¨Ü‡¨∏‡¨ø‡¨≤‡≠á ‡¨Æ‡≠ã ‡¨ú‡≠Ä‡¨¨‡¨®‡¨∞‡≠á\n",
            "Target (English): When you entered my life\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "translate_dataset='/content/drive/MyDrive/ML-DL/Translate/Odia_poetic_sentences.json'\n",
        "# Load the dataset from the JSON file\n",
        "with open(translate_dataset, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Extract source and target texts\n",
        "source_texts = data['source_texts']\n",
        "target_texts = data['target_texts']\n",
        "\n",
        "# Example: Printing the first pair\n",
        "print(\"Source (Odia):\", source_texts[0])\n",
        "print(\"Target (English):\", target_texts[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")"
      ],
      "metadata": {
        "id": "PowuGACOmd7S"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset as TorchDataset\n",
        "\n",
        "class TranslationDataset(TorchDataset):\n",
        "    def __init__(self, source_texts, target_texts, tokenizer, max_length=128):\n",
        "        self.source_texts = source_texts\n",
        "        self.target_texts = target_texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.source_texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        source = self.source_texts[idx]\n",
        "        target = self.target_texts[idx]\n",
        "\n",
        "        # Tokenizing source texts\n",
        "        source_encodings = self.tokenizer(\n",
        "                source,\n",
        "                return_tensors=\"pt\",\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                max_length=self.max_length,  # Ensure the input texts are truncated or padded to max_length\n",
        "                add_special_tokens=True,  # Add special tokens for the encoder-decoder model\n",
        "                padding_side='right'  # Pads to the right (standard for MBart)\n",
        "            )\n",
        "\n",
        "        # Tokenizing target texts\n",
        "        target_encodings = self.tokenizer(\n",
        "                target,\n",
        "                return_tensors=\"pt\",\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                max_length=self.max_length,  # Ensure the input texts are truncated or padded to max_length\n",
        "                add_special_tokens=True,  # Add special tokens for the encoder-decoder model\n",
        "                padding_side='right'  # Pads to the right (standard for MBart)\n",
        "            )\n",
        "\n",
        "        # Returning the tokenized input and output\n",
        "        return {\n",
        "            'input_ids': source_encodings['input_ids'].squeeze(0),  # Remove batch dimension\n",
        "            'attention_mask': source_encodings['attention_mask'].squeeze(0),  # Remove batch dimension\n",
        "            'labels': target_encodings['input_ids'].squeeze(0)  # Labels are also the tokenized target\n",
        "        }\n",
        "\n",
        "# Prepare the train and evaluation datasets\n",
        "train_dataset = TranslationDataset(source_texts, target_texts, tokenizer)\n",
        "eval_dataset = TranslationDataset(source_texts, target_texts, tokenizer)\n"
      ],
      "metadata": {
        "id": "Raju7RbIg3FD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Fine-tuning arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=5,\n",
        "    predict_with_generate=True,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    report_to=None,\n",
        "    run_name=\"Translate-Odia-Eng\",\n",
        "    fp16=True,\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wye-eHDme6SD",
        "outputId": "6c5063d2-0ec1-4e20-f21f-0295a12e850c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "5yUGh33bfOVy",
        "outputId": "8cad8cb3-a6d8-4cd3-ee53-41e4592259b2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-293964a96802>:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 07:32, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.742200</td>\n",
              "      <td>0.005889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.004200</td>\n",
              "      <td>0.000694</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>0.000621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.000576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.000552</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=625, training_loss=0.5999337917834521, metrics={'train_runtime': 452.936, 'train_samples_per_second': 5.52, 'train_steps_per_second': 1.38, 'total_flos': 677228052480000.0, 'train_loss': 0.5999337917834521, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)  # Move model to GPU if available\n",
        "model.eval()\n",
        "\n",
        "input_text = \"‡¨§‡≠Å‡¨Æ‡≠á ‡¨ï‡≠á‡¨â‡¨Å‡¨†‡¨æ‡¨∞‡≠á ‡¨•‡¨æ‡¨ì?\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)  # Move inputs to the same device as the model\n",
        "\n",
        "translated_tokens = model.generate(**inputs)\n",
        "translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
        "print(\"Translation:\", translated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqgZMqdPnvyq",
        "outputId": "37bc9c81-bb30-42c0-b241-2bb8004595c0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Translation: I wish to stay by your side?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('./my_model')  # Replace './my_model' with your preferred directory path\n"
      ],
      "metadata": {
        "id": "yg90T5DgviT7"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}